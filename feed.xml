<?xml version="1.0" encoding="utf-8" ?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>DocuSign Dev Blog</title><atom:link href="undefined/feed.xml" rel="self" type="application/rss+xml"></atom:link><link></link><description>Brought to you by the development teams at DocuSign</description><pubDate>Tue, 12 Nov 2013 16:00:00 -0800</pubDate><generator>Wintersmith - https://github.com/jnordberg/wintersmith</generator><language>en</language><item><title>Building a Redis Sentinel Client for Node.js</title><link>undefined/articles/redis-sentinel-client-nodejs/</link><pubDate>Tue, 12 Nov 2013 16:00:00 -0800</pubDate><guid isPermaLink="true">undefined/articles/redis-sentinel-client-nodejs/</guid><author></author><description>&lt;p&gt;We use &lt;a href=&quot;http://redis.io/&quot;&gt;Redis&lt;/a&gt; for sessions and for a short-lived data cache in our &lt;a href=&quot;http://nodejs.org/&quot;&gt;node.js&lt;/a&gt; application.
Like any component in the system, there&amp;#39;s a potential risk of failure, and graceful failover to a “slave” instance is a way to mitigate the impact. We use &lt;a href=&quot;http://redis.io/topics/sentinel&quot;&gt;&lt;strong&gt;Redis Sentinel&lt;/strong&gt;&lt;/a&gt; to help manage this failover process.&lt;/p&gt;
&lt;p&gt;As the &lt;a href=&quot;http://redis.io/topics/sentinel&quot;&gt;docs&lt;/a&gt; describe,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Redis Sentinel is a &lt;strong&gt;distributed system&lt;/strong&gt;, this means that usually you want to run multiple Sentinel processes across your infrastructure, and this processes will use agreement protocols in order to understand if a master is down and to perform the failover.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Essentially, each node server has its own sentinel corresponding to each redis cluster [master and slave(s)] that it connects to. We have one redis cluster, so &lt;strong&gt;for N node servers, there are N sentinels&lt;/strong&gt;. (This isn&amp;#39;t the only way to do it - there could be only one sentinel, or any other configuration really, but the 1:1 ratio seems to be the simplest.) Each sentinel is connected to the master and slaves to monitor their availability, as well as to the other sentinels. If the master goes down, the sentinels establish a “quorum” and agree on which slave to promote to master. They communicate this through their own pub/sub channels.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The sentinel is not a proxy&lt;/strong&gt; - the connection to the sentinel doesn&amp;#39;t &lt;em&gt;replace&lt;/em&gt; the connecton to the master - it&amp;#39;s a separate instance with the sole purpose of managing master/slave availability. So the app connects to the sentinel in parallel with the master connection, and listens to the chatter on the sentinel channels to know when a failover occurred. It then has to manage the reconnection to the new master on its own.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;/articles/redis-sentinel-client-nodejs/redis-sentinel-diagram.pdf&quot;&gt;&lt;img src=&quot;/articles/redis-sentinel-client-nodejs/redis-sentinel-diagram.png&quot; alt=&quot;Redis Sentinel Client flow diagram&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We&amp;#39;re using the standard &lt;a href=&quot;https://github.com/mranney/node_redis&quot;&gt;node_redis&lt;/a&gt; library, which is robust, easy to use, and works “out of the box” for things like &lt;a href=&quot;https://github.com/visionmedia/connect-redis&quot;&gt;sessions&lt;/a&gt;. But a year ago, when Sentinel started to gain adoption, the best approach for adding &lt;a href=&quot;https://github.com/mranney/node_redis/issues/302&quot;&gt;&lt;strong&gt;Sentinel awareness&lt;/strong&gt;&lt;/a&gt; to node_redis clients wasn&amp;#39;t clear, so a thread started on Github to &lt;a href=&quot;https://github.com/mranney/node_redis/issues/302&quot;&gt;figure it out&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;One simple approach was for the application to simply hold two connections, for sentinel and master, and when the sentinel reports a failover, to reconnect the master. But the way node_redis works, any &lt;strong&gt;data in transit during the failover is lost&lt;/strong&gt;. Also with this approach, the code listening to the Sentinel‘s pub/sub chatter lived in the application, and wasn’t as &lt;strong&gt;encapsulated&lt;/strong&gt; as we thought it should be.&lt;/p&gt;
&lt;p&gt;So we decided to create a middle tier, a &lt;strong&gt;redis sentinel client&lt;/strong&gt;, that would handle all this automatically.
&lt;span class=&quot;more&quot;&gt;&lt;/span&gt;
The goals were:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Transparent, drop-in replacement&lt;/strong&gt; for a &lt;a href=&quot;https://github.com/mranney/node_redis&quot;&gt;node_redis&lt;/a&gt; client, handling connections to master, slave(s), and sentinel in the background.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Handles all RedisClient commands&lt;/strong&gt; (including pub/sub).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;No data loss&lt;/strong&gt; during failover.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The result - still a work in progress - is the &lt;a href=&quot;https://github.com/DocuSignDev/node-redis-sentinel-client&quot;&gt;&lt;strong&gt;node-redis-sentinel-client&lt;/strong&gt;&lt;/a&gt; module. Initially we added it into a fork of node_redis itself, then we split it into its own &lt;a href=&quot;https://npmjs.org/package/redis-sentinel-client&quot;&gt;module&lt;/a&gt;, but still dependent on &lt;a href=&quot;https://github.com/DocuSignDev/node_redis&quot;&gt;our fork&lt;/a&gt; to export shared components and fix the data loss problem.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;RedisSentinelClient&lt;/code&gt; object holds three sub-clients (each a normal &lt;code&gt;RedisClient&lt;/code&gt; object): an &lt;code&gt;activeMasterClient&lt;/code&gt; which connects to the current master, a &lt;code&gt;sentinelTalker&lt;/code&gt; to read from the Sentinel, and a &lt;code&gt;sentinelListener&lt;/code&gt; to listen for failovers (because in node_redis&amp;#39; pubsub mode, a client can only pub &lt;em&gt;or&lt;/em&gt; sub, not both.) All commands get proxied to the &lt;code&gt;activeMasterClient&lt;/code&gt;, and that client is reconnected to the new master after a failover.&lt;/p&gt;
&lt;p&gt;This has worked pretty well so far, including in production. We&amp;#39;ve never actually had a Redis failover in production, fortunately, but in all our tests, the client behaves well: the node processes temporarily lose connectivity, but once the failover completes, they resume gracefully with no data loss.&lt;/p&gt;
&lt;p&gt;There are still a few questions and problems with our solution, however:&lt;/p&gt;
&lt;p&gt;First, when the &lt;code&gt;RedisSentinelClient&lt;/code&gt; is first instantiated, if it can‘t immediately connect, it doesn’t handle it very well. This is because of the way the &lt;code&gt;activeMasterClient&lt;/code&gt; is first set up, and a simple fix has been elusive. (It becomes “stable” only after this initial connection.)&lt;/p&gt;
&lt;p&gt;Second, this middle-tier solution &lt;strong&gt;might ultimately be too heavy&lt;/strong&gt;. Our Redis data is considered volatile: since it&amp;#39;s only for sessions and temporary caching, data loss is at worst a nuisance. So all the effort put into buffering data during a failover might be unnecessary. (On the other hand, Redis supports disk backup, and not every implementation is for volatile data, so a general-purpose solution could err on the side of robustness.)&lt;/p&gt;
&lt;p&gt;Third, the changes in our &lt;a href=&quot;https://github.com/DocuSignDev/node_redis&quot;&gt;fork&lt;/a&gt; to &lt;a href=&quot;https://github.com/mranney/node_redis&quot;&gt;node_redis&lt;/a&gt; (submitted as two &lt;a href=&quot;https://github.com/mranney/node_redis/pull/428&quot;&gt;pull&lt;/a&gt; &lt;a href=&quot;https://github.com/mranney/node_redis/pull/429&quot;&gt;requests&lt;/a&gt;) haven‘t been accepted, probably because there still isn’t consensus on the right approach. It‘s also possible (and a little surprising) that Sentinel itself hasn’t fully caught on. (Surprising because it solves a real problem very nicely, and lacks strong alternatives.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Do you use Redis Sentinel with node? How do you do it?&lt;/strong&gt; We&amp;#39;d love to hear about your experience or ideas in the comments.&lt;/p&gt;
</description></item><item><title>Hackathon!</title><link>undefined/articles/docusign-hackathon-2013/</link><pubDate>Sun, 27 Oct 2013 17:00:00 -0700</pubDate><guid isPermaLink="true">undefined/articles/docusign-hackathon-2013/</guid><author></author><description>&lt;p&gt;&lt;a href=&quot;/articles/docusign-hackathon-2013/hackathon2013-big.jpg&quot;&gt;&lt;img src=&quot;/articles/docusign-hackathon-2013/hackathon2013-big.jpg&quot; alt=&quot;DocuSign Hackathon 2013&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For two days last month, all the DocuSign engineering, design, and product development teams converged on San Francisco for a company hackathon. It was an opportunity to put aside sprint goals and product roadmaps for a little while, think completely out of the box, work with people we don&amp;#39;t usually work with, and build some awesome new features and prototypes.&lt;/p&gt;
&lt;p&gt;Some teams built internal tools to streamline our workflows and automate previously-manual processes. Many of the proofs-of-concept were customer-facing features that will be put on the product roadmap. One team imagined ways &lt;a href=&quot;http://www.html5rocks.com/en/tutorials/getusermedia/intro/&quot;&gt;HTML5 camera support&lt;/a&gt; and real-time chat could be integrated into the product. Another project added real-time editing to our Send flow.
There was a visualization of DocuSign usage with dots all over a 3D globe, that we hope to put on a big screen in the office soon.&lt;/p&gt;
&lt;p&gt;One of the projects was creating this dev blog. So, welcome! We hope to introduce you to our great teams, show you all the cool stuff we build, open-source some of our code, and stimulate many interesting conversations.&lt;/p&gt;
</description></item></channel></rss>